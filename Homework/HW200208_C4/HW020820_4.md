---
title: "HW202008_Ch4"
output: 
  html_document:
    keep_md: TRUE
    toc: TRUE
    toc_float: TRUE
---



## Chapter 4 Applied

###Q10


```r
attach(Weekly)
glimpse(Weekly)
```

```
## Observations: 1,089
## Variables: 9
## $ Year      <dbl> 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990...
## $ Lag1      <dbl> 0.816, -0.270, -2.576, 3.514, 0.712, 1.178, -1.372, ...
## $ Lag2      <dbl> 1.572, 0.816, -0.270, -2.576, 3.514, 0.712, 1.178, -...
## $ Lag3      <dbl> -3.936, 1.572, 0.816, -0.270, -2.576, 3.514, 0.712, ...
## $ Lag4      <dbl> -0.229, -3.936, 1.572, 0.816, -0.270, -2.576, 3.514,...
## $ Lag5      <dbl> -3.484, -0.229, -3.936, 1.572, 0.816, -0.270, -2.576...
## $ Volume    <dbl> 0.1549760, 0.1485740, 0.1598375, 0.1616300, 0.153728...
## $ Today     <dbl> -0.270, -2.576, 3.514, 0.712, 1.178, -1.372, 0.807, ...
## $ Direction <fct> Down, Down, Up, Up, Up, Down, Up, Up, Up, Down, Down...
```

####a)


```r
cor(Weekly[, -9])
```

```
##               Year         Lag1        Lag2        Lag3         Lag4
## Year    1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923
## Lag1   -0.03228927  1.000000000 -0.07485305  0.05863568 -0.071273876
## Lag2   -0.03339001 -0.074853051  1.00000000 -0.07572091  0.058381535
## Lag3   -0.03000649  0.058635682 -0.07572091  1.00000000 -0.075395865
## Lag4   -0.03112792 -0.071273876  0.05838153 -0.07539587  1.000000000
## Lag5   -0.03051910 -0.008183096 -0.07249948  0.06065717 -0.075675027
## Volume  0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617
## Today  -0.03245989 -0.075031842  0.05916672 -0.07124364 -0.007825873
##                Lag5      Volume        Today
## Year   -0.030519101  0.84194162 -0.032459894
## Lag1   -0.008183096 -0.06495131 -0.075031842
## Lag2   -0.072499482 -0.08551314  0.059166717
## Lag3    0.060657175 -0.06928771 -0.071243639
## Lag4   -0.075675027 -0.06107462 -0.007825873
## Lag5    1.000000000 -0.05851741  0.011012698
## Volume -0.058517414  1.00000000 -0.033077783
## Today   0.011012698 -0.03307778  1.000000000
```

```r
pairs(Weekly)
```

![](HW020820_4_files/figure-html/unnamed-chunk-2-1.png)<!-- -->
Only the correlation between year and volume is quite significant. Volume increases over time.

```r
plot(Year, Volume)
```

![](HW020820_4_files/figure-html/unnamed-chunk-3-1.png)<!-- -->


```r
plot(Volume)
```

![](HW020820_4_files/figure-html/unnamed-chunk-4-1.png)<!-- -->

####b)


```r
fit.glm <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(fit.glm)
```

```
## 
## Call:
## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 
##     Volume, family = binomial, data = Weekly)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6949  -1.2565   0.9913   1.0849   1.4579  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)   
## (Intercept)  0.26686    0.08593   3.106   0.0019 **
## Lag1        -0.04127    0.02641  -1.563   0.1181   
## Lag2         0.05844    0.02686   2.175   0.0296 * 
## Lag3        -0.01606    0.02666  -0.602   0.5469   
## Lag4        -0.02779    0.02646  -1.050   0.2937   
## Lag5        -0.01447    0.02638  -0.549   0.5833   
## Volume      -0.02274    0.03690  -0.616   0.5377   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1496.2  on 1088  degrees of freedom
## Residual deviance: 1486.4  on 1082  degrees of freedom
## AIC: 1500.4
## 
## Number of Fisher Scoring iterations: 4
```

####c)


```r
probs <- predict(fit.glm, type = "response")
pred.glm <- rep("Down", length(probs))
pred.glm[probs > 0.5] <- "Up"
table(pred.glm, Direction)
```

```
##         Direction
## pred.glm Down  Up
##     Down   54  48
##     Up    430 557
```


```r
right = (54+557)/(54+48+430+557)
right
```

```
## [1] 0.5610652
```

####d) logistic regression


```r
train <- (Year < 2009)
test <- Weekly[!train, ]
test_direction <- Direction[!train]
fit.glm2 <- glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
summary(fit.glm2)
```

```
## 
## Call:
## glm(formula = Direction ~ Lag2, family = binomial, data = Weekly, 
##     subset = train)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.536  -1.264   1.021   1.091   1.368  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)   
## (Intercept)  0.20326    0.06428   3.162  0.00157 **
## Lag2         0.05810    0.02870   2.024  0.04298 * 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1354.7  on 984  degrees of freedom
## Residual deviance: 1350.5  on 983  degrees of freedom
## AIC: 1354.5
## 
## Number of Fisher Scoring iterations: 4
```


```r
probs2 <- predict(fit.glm2, test, type = "response")
pred.glm2 <- rep("Down", length(probs2))
pred.glm2[probs2 > 0.5] <- "Up"
table(pred.glm2, test_direction)
```

```
##          test_direction
## pred.glm2 Down Up
##      Down    9  5
##      Up     34 56
```


```r
1-(9+56)/(9+5+34+56)
```

```
## [1] 0.375
```
test error is 0.375

####e) LDA


```r
fit.lda <- lda(Direction ~ Lag2, data = Weekly, subset = train)
fit.lda
```

```
## Call:
## lda(Direction ~ Lag2, data = Weekly, subset = train)
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4477157 0.5522843 
## 
## Group means:
##             Lag2
## Down -0.03568254
## Up    0.26036581
## 
## Coefficients of linear discriminants:
##            LD1
## Lag2 0.4414162
```


```r
pred.lda <- predict(fit.lda, test)
table(pred.lda$class, test_direction)
```

```
##       test_direction
##        Down Up
##   Down    9  5
##   Up     34 56
```
the same with logistic regression model.

####f) QDA


```r
fit.qda <- qda(Direction ~ Lag2, data = Weekly, subset = train)
fit.qda
```

```
## Call:
## qda(Direction ~ Lag2, data = Weekly, subset = train)
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4477157 0.5522843 
## 
## Group means:
##             Lag2
## Down -0.03568254
## Up    0.26036581
```


```r
pred.qda <- predict(fit.qda, test)
table(pred.qda$class, test_direction)
```

```
##       test_direction
##        Down Up
##   Down    0  0
##   Up     43 61
```
Always predict Up.

```r
1-61/(43+61)
```

```
## [1] 0.4134615
```

####g) KNN, K=1


```r
train.X <- as.matrix(Lag2[train])
test.X <- as.matrix(Lag2[!train])
train_direction <- Direction[train]
set.seed(1)
pred.knn <- knn(train.X, test.X, train_direction, k = 1)
table(pred.knn, test_direction)
```

```
##         test_direction
## pred.knn Down Up
##     Down   21 30
##     Up     22 31
```


```r
1-(21+31)/(21+30+22+31)
```

```
## [1] 0.5
```
error rate 0.5

####h) logistic = LDA > QDA > KNN

###Q11


```r
attach(Auto)
glimpse(Auto)
```

```
## Observations: 392
## Variables: 9
## $ mpg          <dbl> 18, 15, 18, 16, 17, 15, 14, 14, 14, 15, 15, 14, 1...
## $ cylinders    <dbl> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 6, 6...
## $ displacement <dbl> 307, 350, 318, 304, 302, 429, 454, 440, 455, 390,...
## $ horsepower   <dbl> 130, 165, 150, 150, 140, 198, 220, 215, 225, 190,...
## $ weight       <dbl> 3504, 3693, 3436, 3433, 3449, 4341, 4354, 4312, 4...
## $ acceleration <dbl> 12.0, 11.5, 11.0, 12.0, 10.5, 10.0, 9.0, 8.5, 10....
## $ year         <dbl> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 7...
## $ origin       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1...
## $ name         <fct> chevrolet chevelle malibu, buick skylark 320, ply...
```

####a)


```r
mpg01 <- rep(0, length(mpg))
mpg01[mpg > median(mpg)] <- 1
Auto1 <- data.frame(Auto, mpg01)
```


```r
cor(Auto1[, -9])
```

```
##                     mpg  cylinders displacement horsepower     weight
## mpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442
## cylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273
## displacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944
## horsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377
## weight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000
## acceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392
## year          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199
## origin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054
## mpg01         0.8369392 -0.7591939   -0.7534766 -0.6670526 -0.7577566
##              acceleration       year     origin      mpg01
## mpg             0.4233285  0.5805410  0.5652088  0.8369392
## cylinders      -0.5046834 -0.3456474 -0.5689316 -0.7591939
## displacement   -0.5438005 -0.3698552 -0.6145351 -0.7534766
## horsepower     -0.6891955 -0.4163615 -0.4551715 -0.6670526
## weight         -0.4168392 -0.3091199 -0.5850054 -0.7577566
## acceleration    1.0000000  0.2903161  0.2127458  0.3468215
## year            0.2903161  1.0000000  0.1815277  0.4299042
## origin          0.2127458  0.1815277  1.0000000  0.5136984
## mpg01           0.3468215  0.4299042  0.5136984  1.0000000
```


```r
pairs(Auto1)
```

![](HW020820_4_files/figure-html/unnamed-chunk-21-1.png)<!-- -->

####b) Cylinders, weight, displacement, horsepower.

####c)


```r
train <- (year %% 2 == 0)
Auto.train <- Auto[train, ]
Auto.test <- Auto[!train, ]
mpg01.test <- mpg01[!train]
```

####d) lda


```r
fit.lda <- lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, subset = train)
fit.lda
```

```
## Call:
## lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, 
##     subset = train)
## 
## Prior probabilities of groups:
##         0         1 
## 0.4571429 0.5428571 
## 
## Group means:
##   cylinders   weight displacement horsepower
## 0  6.812500 3604.823     271.7396  133.14583
## 1  4.070175 2314.763     111.6623   77.92105
## 
## Coefficients of linear discriminants:
##                        LD1
## cylinders    -0.6741402638
## weight       -0.0011465750
## displacement  0.0004481325
## horsepower    0.0059035377
```


```r
pred.lda <- predict(fit.lda, Auto.test)
table(pred.lda$class, mpg01.test)
```

```
##    mpg01.test
##      0  1
##   0 86  9
##   1 14 73
```


```r
mean(pred.lda$class != mpg01.test)
```

```
## [1] 0.1263736
```

####e) qda


```r
fit.qda <- qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, subset = train)
fit.qda
```

```
## Call:
## qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, 
##     subset = train)
## 
## Prior probabilities of groups:
##         0         1 
## 0.4571429 0.5428571 
## 
## Group means:
##   cylinders   weight displacement horsepower
## 0  6.812500 3604.823     271.7396  133.14583
## 1  4.070175 2314.763     111.6623   77.92105
```


```r
pred.qda <- predict(fit.qda, Auto.test)
table(pred.qda$class, mpg01.test)
```

```
##    mpg01.test
##      0  1
##   0 89 13
##   1 11 69
```

```r
mean(pred.qda$class != mpg01.test)
```

```
## [1] 0.1318681
```

####f) logistic


```r
fit.glm <- glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, family = binomial, subset = train)
summary(fit.glm)
```

```
## 
## Call:
## glm(formula = mpg01 ~ cylinders + weight + displacement + horsepower, 
##     family = binomial, data = Auto, subset = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.48027  -0.03413   0.10583   0.29634   2.57584  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  17.658730   3.409012   5.180 2.22e-07 ***
## cylinders    -1.028032   0.653607  -1.573   0.1158    
## weight       -0.002922   0.001137  -2.569   0.0102 *  
## displacement  0.002462   0.015030   0.164   0.8699    
## horsepower   -0.050611   0.025209  -2.008   0.0447 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 289.58  on 209  degrees of freedom
## Residual deviance:  83.24  on 205  degrees of freedom
## AIC: 93.24
## 
## Number of Fisher Scoring iterations: 7
```


```r
probs <- predict(fit.glm, Auto.test, type = "response")
pred.glm <- rep(0, length(probs))
pred.glm[probs > 0.5] <- 1
table(pred.glm, mpg01.test)
```

```
##         mpg01.test
## pred.glm  0  1
##        0 89 11
##        1 11 71
```

```r
mean(pred.glm != mpg01.test)
```

```
## [1] 0.1208791
```

####g) KNN


```r
train.X <- cbind(cylinders, weight, displacement, horsepower)[train, ]
test.X <- cbind(cylinders, weight, displacement, horsepower)[!train, ]
train.mpg01 <- mpg01[train]
```

K=1

```r
set.seed(1)
pred.knn <- knn(train.X, test.X, train.mpg01, k = 1)
table(pred.knn, mpg01.test)
```

```
##         mpg01.test
## pred.knn  0  1
##        0 83 11
##        1 17 71
```

```r
mean(pred.knn != mpg01.test)
```

```
## [1] 0.1538462
```

K=5

```r
set.seed(1)
pred.knn5 <- knn(train.X, test.X, train.mpg01, k = 5)
table(pred.knn5, mpg01.test)
```

```
##          mpg01.test
## pred.knn5  0  1
##         0 82  9
##         1 18 73
```

```r
mean(pred.knn5 != mpg01.test)
```

```
## [1] 0.1483516
```

K=10

```r
set.seed(1)
pred.knn10 <- knn(train.X, test.X, train.mpg01, k = 10)
table(pred.knn10, mpg01.test)
```

```
##           mpg01.test
## pred.knn10  0  1
##          0 79  7
##          1 21 75
```

```r
mean(pred.knn10 != mpg01.test)
```

```
## [1] 0.1538462
```

###Q12

####a)


```r
power <- function(){
  2^3
}
power()
```

```
## [1] 8
```

####b)


```r
power2 <- function(x, a){
  x^a
}
power2(2, 3)
```

```
## [1] 8
```

####c)


```r
power2(10, 3)
```

```
## [1] 1000
```

```r
power2(8, 17)
```

```
## [1] 2.2518e+15
```

```r
power2(131, 3)
```

```
## [1] 2248091
```

####d)


```r
power3 <- function(x , a) {
    result <- x^a
    return(result)
}
power3(2, 3)
```

```
## [1] 8
```

####e)


```r
x <- 1:10
plot(x, power3(x, 3))
```

![](HW020820_4_files/figure-html/unnamed-chunk-38-1.png)<!-- -->

```r
plot(x, power3(x, 3), log = "y")
```

![](HW020820_4_files/figure-html/unnamed-chunk-38-2.png)<!-- -->

####f)


```r
plotpower <- function(x, a){
  plot(x, power3(x, a))
}
plotpower(1:10, 3)
```

![](HW020820_4_files/figure-html/unnamed-chunk-39-1.png)<!-- -->

###Q13

####a)


```r
attach(Boston)
crim01 <- rep(0, length(crim))
crim01[crim > median(crim)] <- 1
boston <- data.frame(Boston, crim01)
```

train and test

```r
train <- 1:(length(crim)*2/3)
test <- (length(crim)*2/3 + 1):length(crim)
boston.train <- boston[train, ]
boston.test <- boston[test, ]
crim01.test <- crim01[test]
```

####logistic


```r
fit.glm <- glm(crim01 ~ . - crim01 - crim, data = Boston, subset = train)
summary(fit.glm)
```

```
## 
## Call:
## glm(formula = crim01 ~ . - crim01 - crim, data = Boston, subset = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.77571  -0.26714  -0.06844   0.23625   0.94890  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -2.5678266  0.5596886  -4.588 6.41e-06 ***
## zn          -0.0001971  0.0011429  -0.172 0.863161    
## indus       -0.0003276  0.0049298  -0.066 0.947063    
## chas         0.0250178  0.0731092   0.342 0.732424    
## nox          2.6587478  0.3751145   7.088 8.61e-12 ***
## rm           0.0371482  0.0727191   0.511 0.609809    
## age          0.0025683  0.0011567   2.220 0.027094 *  
## dis          0.0121048  0.0190689   0.635 0.526012    
## rad          0.0340761  0.0135968   2.506 0.012696 *  
## tax          0.0005062  0.0003609   1.403 0.161685    
## ptratio      0.0424540  0.0118146   3.593 0.000377 ***
## black       -0.0006539  0.0005271  -1.240 0.215701    
## lstat        0.0027107  0.0055736   0.486 0.627051    
## medv         0.0107270  0.0062470   1.717 0.086910 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 0.12274)
## 
##     Null deviance: 76.682  on 336  degrees of freedom
## Residual deviance: 39.645  on 323  degrees of freedom
## AIC: 265.15
## 
## Number of Fisher Scoring iterations: 2
```


```r
fit.glm1 <- glm(crim01 ~ nox + age + rad + ptratio, data = Boston, subset = train)
summary(fit.glm1)
```

```
## 
## Call:
## glm(formula = crim01 ~ nox + age + rad + ptratio, data = Boston, 
##     subset = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7596  -0.2781  -0.0728   0.3153   0.8690  
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -1.6180267  0.2207052  -7.331 1.75e-12 ***
## nox          2.4509863  0.2636217   9.297  < 2e-16 ***
## age          0.0024638  0.0009373   2.629 0.008974 ** 
## rad          0.0468901  0.0122563   3.826 0.000156 ***
## ptratio      0.0197521  0.0092238   2.141 0.032966 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 0.1266088)
## 
##     Null deviance: 76.682  on 336  degrees of freedom
## Residual deviance: 42.034  on 332  degrees of freedom
## AIC: 266.86
## 
## Number of Fisher Scoring iterations: 2
```


```r
probs <- predict(fit.glm1, boston.test, "response")
pred.glm1 <- rep(0, length(probs))
pred.glm1[probs > 0.5] <- 1
table(pred.glm1, crim01.test)
```

```
##          crim01.test
## pred.glm1   0   1
##         0  23   0
##         1  10 135
```

####lda


```r
fit.lda1 <- lda(crim01 ~ nox + age + rad + ptratio, data = boston.train)
pred.lda1 <- predict(fit.lda1, boston.test)
table(pred.lda1$class, crim01.test)
```

```
##    crim01.test
##       0   1
##   0  23   0
##   1  10 135
```

####qda


```r
fit.qda1 <- qda(crim01 ~ nox + age + rad + ptratio, data = boston.train)
pred.qda1 <- predict(fit.qda1, boston.test)
table(pred.qda1$class, crim01.test)
```

```
##    crim01.test
##       0   1
##   0  19 132
##   1  14   3
```

####KNN


```r
train.c <- cbind(nox, age, rad, ptratio)[train, ]
test.c <- cbind(nox, age, rad, ptratio)[test, ]
train.crim01 <- crim01[train]
```

k=1

```r
set.seed(121)
pred.knn1 <- knn(train.c, test.c, train.crim01, k = 1)
table(pred.knn1, crim01.test)
```

```
##          crim01.test
## pred.knn1  0  1
##         0 26 94
##         1  7 41
```
k=9

```r
pred.knn9 <- knn(train.c, test.c, train.crim01, k = 9)
table(pred.knn9, crim01.test)
```

```
##          crim01.test
## pred.knn9  0  1
##         0 26 57
##         1  7 78
```

k=15

```r
pred.knn15 <- knn(train.c, test.c, train.crim01, k = 15)
table(pred.knn15, crim01.test)
```

```
##           crim01.test
## pred.knn15  0  1
##          0 26 64
##          1  7 71
```

####compare


```r
mean(pred.glm1 != crim01.test)
```

```
## [1] 0.05952381
```

```r
mean(pred.lda1$class != crim01.test)
```

```
## [1] 0.05952381
```

```r
mean(pred.qda1$class != crim01.test)
```

```
## [1] 0.8690476
```

```r
mean(pred.knn9 != crim01.test)
```

```
## [1] 0.3809524
```


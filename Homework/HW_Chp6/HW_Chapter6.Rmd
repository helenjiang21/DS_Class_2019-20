---
title: "Chapter 6 HW"
output: 
  html_document:
    keep_md: TRUE
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(leaps)
library(tidyverse)
library(glmnet)
library(ISLR)
library(pls)
library(MASS)
```

##Q8

###a

```{r}
set.seed(123)
X <- rnorm(100)
eps <- rnorm(100)
```

###b

```{r}
b0 <- 1
b1 <- 20
b2 <- -3
b3 <- 4
Y<- b0 + b1 * X + b2 * X^2 + b3 * X^3 + eps
```

###c subset

```{r}
df <- data.frame(x=X, y=Y)
reg_fit <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = df, nvmax = 10)
reg_sum <- summary(reg_fit)
reg_sum
```

```{r}
plot(reg_sum$cp, xlab = "variables", ylab = "Cp", type = "l")
points(which.min(reg_sum$cp), reg_sum$cp[which.min(reg_sum$cp)], col = "red", cex = 2, pch = 20)
```

```{r}
plot(reg_sum$bic, xlab = "variables", ylab = "BIC", type = "l")
points(which.min(reg_sum$bic), reg_sum$bic[which.min(reg_sum$bic)], col = "red", cex = 2, pch = 20)
```

```{r}
plot(reg_sum$adjr2, xlab = "variables", ylab = "adjr2", type = "l")
points(which.max(reg_sum$adjr2), reg_sum$adjr2[which.max(reg_sum$adjr2)], col = "red", cex = 2, pch = 20)
```
Why not at 3 but 6?
```{r}
reg_sum$adjr2
```
Adjusted r squared for 3-10 are fairly close.

Picking the model with 3 variables.
```{r}
coef(reg_fit, which.min(reg_sum$cp))
```

###d stepwise

forward first:
```{r}
reg_fwd <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = df, nvmax = 10, method = "forward")
regfwd_sum <- summary(reg_fwd)
```

```{r}
plot(regfwd_sum$cp, xlab = "variables", ylab = "Cp", type = "l")
points(which.min(regfwd_sum$cp), regfwd_sum$cp[which.min(regfwd_sum$cp)], col = "red", cex = 2, pch = 20)

plot(regfwd_sum$bic, xlab = "variables", ylab = "BIC", type = "l")
points(which.min(regfwd_sum$bic), regfwd_sum$bic[which.min(regfwd_sum$bic)], col = "red", cex = 2, pch = 20)

plot(regfwd_sum$adjr2, xlab = "variables", ylab = "adjr2", type = "l")
points(which.max(regfwd_sum$adjr2), regfwd_sum$adjr2[which.max(regfwd_sum$adjr2)], col = "red", cex = 2, pch = 20)
```

Choosing the model with 3 variables:
```{r}
coef(reg_fwd, which.min(regfwd_sum$cp))
```

then backward:
```{r}
reg_bwd <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = df, nvmax = 10, method = "backward")
regbwd_sum <- summary(reg_bwd)
```

```{r}
plot(regbwd_sum$cp, xlab = "variables", ylab = "Cp", type = "l")
points(which.min(regbwd_sum$cp), regbwd_sum$cp[which.min(regbwd_sum$cp)], col = "red", cex = 2, pch = 20)

plot(regbwd_sum$bic, xlab = "variables", ylab = "BIC", type = "l")
points(which.min(regbwd_sum$bic), regbwd_sum$bic[which.min(regbwd_sum$bic)], col = "red", cex = 2, pch = 20)

plot(regbwd_sum$adjr2, xlab = "variables", ylab = "adjr2", type = "l")
points(which.max(regbwd_sum$adjr2), regbwd_sum$adjr2[which.max(regbwd_sum$adjr2)], col = "red", cex = 2, pch = 20)
```

Choosing the one with 6 variables.
```{r}
coef(reg_bwd, which.min(regbwd_sum$cp))
```

###e. lasso

```{r}
xmat <- model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = df)[, -1]
cv_lasso <- cv.glmnet(xmat, Y, alpha = 1)
plot(cv_lasso)
```

```{r}
bestlam <- cv_lasso$lambda.min
bestlam
```

```{r}
lasso_fit <- glmnet(xmat, Y, alpha = 1)
predict(lasso_fit, s = bestlam, type = "coefficients")[1:11, ]
```
A model with three variables.

###f. 

```{r}
b7 <- 7
y <- b0 + b7 * X^7 + eps
df1 <- data.frame(y = y, x = X)
reg_fit1 <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = df1, nvmax = 10)
reg_sum1 <- summary(reg_fit1)
par(mfrow = c(2, 2))
plot(reg_sum1$cp, xlab = "variables", ylab = "C_p", type = "l")
points(which.min(reg_sum1$cp), reg_sum1$cp[which.min(reg_sum1$cp)], col = "red", cex = 2, pch = 20)
plot(reg_sum1$bic, xlab = "variables", ylab = "BIC", type = "l")
points(which.min(reg_sum1$bic), reg_sum1$bic[which.min(reg_sum1$bic)], col = "red", cex = 2, pch = 20)
plot(reg_sum1$adjr2, xlab = "variables", ylab = "Adjusted R^2", type = "l")
points(which.max(reg_sum1$adjr2), reg_sum1$adjr2[which.max(reg_sum1$adjr2)], col = "red", cex = 2, pch = 20)
```

```{r}
coef(reg_fit1, 1)
coef(reg_fit1, 6)
```

Cp and BIC picks the most accurate one.

```{r}
xmat1 <- model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = df1)[, -1]
cv_lasso1 <- cv.glmnet(xmat1, y, alpha = 1)
bestlam1 <- cv_lasso1$lambda.min
bestlam1
```

```{r}
lasso_fit1 <- glmnet(xmat1, y, alpha = 1)
predict(lasso_fit1, s = bestlam1, type = "coefficients")[1:11, ]
```
Lasso also picks the most accurate one.

##Q9 

###a.. splitting

```{r}
attach(College)
set.seed(134)
train = sample(1:dim(College)[1], dim(College)[1] / 2)
test <- -train
college_train <- College[train, ]
college_test <- College[test, ]
```

###b. lm

```{r}
lm_fit <- lm(Apps ~ ., data = college_train)
lm_pred <- predict(lm_fit, college_test)
mean((lm_pred - college_test$Apps)^2)
```

###c. ridge w/ cv

```{r}
train.mat <- model.matrix(Apps ~ ., data = college_train)
test.mat <- model.matrix(Apps ~ ., data = college_test)
grid <- 10 ^ seq(4, -2, length = 100)
ridge_fit <- glmnet(train.mat, college_train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv_ridge <- cv.glmnet(train.mat, college_train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlam_ridge <- cv_ridge$lambda.min
bestlam_ridge
```

```{r}
ridge_pred <- predict(ridge_fit, s = bestlam_ridge, newx = test.mat)
mean((ridge_pred - college_test$Apps)^2)
```
a bit lower that least square

###d. lasso w/ cv

```{r}
lasso_fit <- glmnet(train.mat, college_train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
cv_lasso <- cv.glmnet(train.mat, college_train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
bestlam_lasso <- cv_lasso$lambda.min
bestlam_lasso
```

```{r}
lasso_pred <- predict(lasso_fit, s = bestlam_lasso, newx = test.mat)
mean((lasso_pred - college_test$Apps)^2)
```
A bit lower.

###e. PCR

```{r}
pcr_fit <- pcr(Apps ~ ., data = college_train, scale = TRUE, validation = "CV")
pcr_pred <- predict(pcr_fit, college_test, ncomp = 10)
mean((pcr_pred - college_test$Apps)^2)
```
This one is higher

###f. pls

```{r}
pls_fit <- plsr(Apps ~ ., data = college_train, scale = TRUE, validation = "CV")
pls_pred <- predict(pls_fit, college_test, ncomp = 10)
mean((pls_pred - college_test$Apps)^2)
```
Higher.

####g. comparison

Linear models --> compare R^2

```{r}
test_mean <- mean(college_test$Apps)
1 - mean((lm_pred - college_test$Apps)^2) / mean((test_mean - college_test$Apps)^2)
1 - mean((ridge_pred - college_test$Apps)^2) / mean((test_mean - college_test$Apps)^2)
1 - mean((lasso_pred - college_test$Apps)^2) / mean((test_mean - college_test$Apps)^2)
1 - mean((pcr_pred - college_test$Apps)^2) / mean((test_mean - college_test$Apps)^2)
1 - mean((pls_pred - college_test$Apps)^2) / mean((test_mean - college_test$Apps)^2)
```

PCR is a bit not as accurate as the others.

##Q10

###a. generate data set

```{r}
set.seed(167)
x <- matrix(rnorm(1000 * 20), 1000, 20)
b <- rnorm(20)
b[1] <- 0
b[5] <- 0
b[16] <- 0
b[17] <- 0
eps <- rnorm(1000)
y <- x %*% b + eps
```

###b. splitting

```{r}
train <- sample(seq(1000), 100, replace = FALSE)
test <- -train
x.train <- x[train, ]
x.test <- x[test, ]
y.train <- y[train]
y.test <- y[test]
```

###c. training

```{r}
df_train <- data.frame(y = y.train, x = x.train)
reg_fit <- regsubsets(y ~ ., data = df_train, nvmax = 20)
train.mat <- model.matrix(y ~ ., data = df_train, nvmax = 20)
train.error <- rep(NA, 20)
for (i in 1:20) {
    coef <- coef(reg_fit, id = i)
    pred <- train.mat[, names(coef)] %*% coef
    train.error[i] <- mean((pred - y.train)^2)
}
plot(train.error, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
```

###d. testing

```{r}
df_test <- data.frame(y = y.test, x = x.test)
test.mat <- model.matrix(y ~ ., data = df_test, nvmax = 20)
test.error <- rep(NA, 20)
for (i in 1:20) {
    coef <- coef(reg_fit, id = i)
    pred <- test.mat[, names(coef)] %*% coef
    test.error[i] <- mean((pred - y.test)^2)
}
plot(test.error, xlab = "Number of predictors", ylab = "Testing MSE", pch = 19, type = "b")
```

###e.

```{r}
which.min(test.error)
```

###f. 

```{r}
coef(reg_fit, which.min(test.error))
```
there is no zero.

###g. **

```{r}
val.errors <- rep(NA, 20)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:20) {
    coefi <- coef(reg_fit, id = i)
    val.errors[i] <- sqrt(sum((b[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) + sum(b[!(x_cols %in% names(coefi))])^2)
}
plot(val.errors, xlab = "Number of coefficients", ylab = "Error between estimated and true coefficients", pch = 19, type = "b")
```

```{r}
coef(reg_fit, which.min(val.errors))
coef(reg_fit, 8)
```
b1, 5, 16, 17 are zeros.
So whether the coefficients fit the actual ones doesn't imply a lower test mse.

##Q11

###a. 

need more time to understand this one..

```{r}
data(Boston)
set.seed(1)

predict.regsubsets <- function(object, newdata, id, ...) {
    form <- as.formula(object$call[[2]])
    mat <- model.matrix(form, newdata)
    coefi <- coef(object, id = id)
    xvars <- names(coefi)
    mat[, xvars] %*% coefi
}

k = 10
folds <- sample(1:k, nrow(Boston), replace = TRUE)
cv.errors <- matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for (j in 1:k) {
    best.fit <- regsubsets(crim ~ ., data = Boston[folds != j, ], nvmax = 13)
    for (i in 1:13) {
        pred <- predict(best.fit, Boston[folds == j, ], id = i)
        cv.errors[j, i] <- mean((Boston$crim[folds == j] - pred)^2)
    }
}
mean.cv.errors <- apply(cv.errors, 2, mean)
plot(mean.cv.errors, type = "b", xlab = "Number of variables", ylab = "CV error")
```

lasso
```{r}
x <- model.matrix(crim ~ ., Boston)[, -1]
y <- Boston$crim
cv.out <- cv.glmnet(x, y, alpha = 1, type.measure = "mse")
plot(cv.out)
```

ridge
```{r}
grid <- 10 ^ seq(4, -2, length = 100)
ridge_fit <- glmnet(x, y, alpha = 0, lambda = grid, thresh = 1e-12)
cv.out <- cv.glmnet(x, y, alpha = 0, type.measure = "mse")
plot(cv.out)
```

pcr
```{r}
pcr.fit <- pcr(crim ~ ., data = Boston, scale = TRUE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")
```

###b.

```{r}
mean(mean.cv.errors)

pcr_pred <- predict(pcr.fit, Boston)
mean((pcr_pred - Boston$crim)^2)
```

###c.

it has 13 variables.
